{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e275fa3-9d1f-40f7-b9bf-d3f4e281b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data in characters\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# ## A small ChatGPT style Transformer\n",
    "# * concepts in NLP and Transformers \n",
    "# * generative mdoels\n",
    "# https://github.com/rcalix1/PyTorch/blob/main/DeepLearning/Transformers/GPTs/GenerativeTransformerTinyGPT.py\n",
    "\n",
    "############################################################ \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import requests\n",
    "## import tiktoken\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "import time\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "############################################################\n",
    "\n",
    "## !pip install requests\n",
    "## !pip install tiktoken    ## requires python   >    3.9\n",
    "\n",
    "############################################################\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "############################################################\n",
    "\n",
    "input_file_path = 'input.txt'\n",
    "\n",
    "## data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "############################################################\n",
    "\n",
    "print(\"length of data in characters\")\n",
    "len(text)\n",
    "\n",
    "############################################################\n",
    "\n",
    "chars = sorted(     list(set(text))   )\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(  ''.join(chars)  )\n",
    "\n",
    "############################################################# \n",
    "## tokenizer\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(chars) }\n",
    "itos = { i:ch for i, ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [ stoi[c]          for c in s   ]    ## encoder: string to integer\n",
    "decode = lambda l: ''.join(   itos[i] for i in l   )    ## decoder: interger to string\n",
    "\n",
    "#############################################################\n",
    "\n",
    "data = torch.tensor(   encode(text), dtype=torch.long   )\n",
    "n    = int(   0.9*len(data)   )\n",
    "train_data = data[:n]\n",
    "val_data   = data[n:]\n",
    "\n",
    "#############################################################\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "    ix = torch.randint(   len(data) - block_size, (batch_size,)   )\n",
    "    x  = torch.stack(    [  data[ i : i+block_size ]   for i in ix]    ) \n",
    "    y  = torch.stack(    [  data[ i+1 : i+1+block_size ]   for i in ix]    )\n",
    "    \n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "############################################################\n",
    "\n",
    "@torch.no_grad()    ## for efficiency\n",
    "def estimate_loss(only_validation=False):\n",
    "    out = {}\n",
    "    model.eval()   ## no training\n",
    "    if only_validation:\n",
    "        arr=[ 'val']\n",
    "    else:\n",
    "        arr=['train', 'val']\n",
    "    for split in arr:\n",
    "        print(\"split\",split)\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            # print(k,range(eval_iters))\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()  ## back to training\n",
    "    return out\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        \n",
    "        ## the mask tril is not part of the graph since only for masking\n",
    "        ## so register buffer makes it a thing out of the graph\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)              ## (B, T, C)\n",
    "        q = self.query(x)            ## (B, T, C)\n",
    "        \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5       ## (B, T, C) @ (B, C, T)  -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))     ## (B, T, T)\n",
    "        wei = F.softmax(wei, dim= -1)           ## (B, T, T)\n",
    "        wei = self.dropout(   wei   )\n",
    "        \n",
    "        ## perform the weighted aggregation of the values\n",
    "        v   = self.value(  x  )   ## (B, T, C)\n",
    "        out = wei @ v             ## (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(  [Head(head_size) for _ in range(num_heads) ] )\n",
    "        self.proj  = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat(   [ h(x) for h in self.heads], dim = -1   )\n",
    "        out = self.proj(  out   )\n",
    "        out = self.dropout(   out   )\n",
    "        return out\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: comuunication followed by computation \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa   = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward( n_embd)\n",
    "        self.ln1  = nn.LayerNorm(n_embd)\n",
    "        self.ln2  = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## these normalizations (ln1, ln2) are about the only thing different from\n",
    "        ## the original Vaswani paper. In the paper, they are done at the end of forward\n",
    "        ## but now they are usually done at the beginning of forward\n",
    "        x = x + self.sa(     self.ln1(x)      )\n",
    "        x = x + self.ffwd(   self.ln2(x)      )\n",
    "        return x\n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)     ## positional encoding \n",
    "        self.blocks = nn.Sequential(\n",
    "                *[   Block(n_embd, n_head=n_head) for _ in range(n_layer)    ]\n",
    "        )\n",
    "        self.ln_f    = nn.LayerNorm(  n_embd    )        ## final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        \n",
    "        ## ids and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)      ## batch, time, embed (4, 8, 32) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))      ## (T, C)\n",
    "        x = tok_emb + pos_emb    ## (B, T, C)\n",
    "        x = self.blocks(  x  )   ## (B, T, C)        \n",
    "        x = self.ln_f(x)         ## (B, T, C)\n",
    "        logits = self.lm_head(x)                 ## (B, T, vocab_sice)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits  = logits.view(B*T, C)\n",
    "            targets  = targets.view(B*T)\n",
    "            loss   = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        ## idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            ## crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            ## get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            ## focus only on last time stamp\n",
    "            logits = logits[:, -1, :]           ## becomes (B, C)\n",
    "            ## apply softmax to get probs\n",
    "            probs = F.softmax(logits, dim= -1)    ## (B, C)\n",
    "            ## sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)     ## (B, 1)\n",
    "            ## append sample to the running sequence\n",
    "            idx = torch.cat(  (idx, idx_next), dim=1  )            ## (B, T+1)\n",
    "        return idx\n",
    "            \n",
    "            \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def calc_compression(model):\n",
    "    total=0\n",
    "    total_zero=0\n",
    "    for p in model.parameters(): \n",
    "        if p.requires_grad:\n",
    "            total+=p.numel()\n",
    "            total_zero+=(p.numel()-torch.count_nonzero(p).item())\n",
    "    return total_zero/total\n",
    "\n",
    "            \n",
    "\n",
    "def set_zero(model):\n",
    "    for p in model.parameters(): \n",
    "        if p.requires_grad:\n",
    "            p.data=torch.zeros(p.shape)\n",
    "\n",
    "def sum_till_n(n):\n",
    "    sum_val=0\n",
    "    for i in range(n):\n",
    "        sum_val+=i\n",
    "    return sum_val\n",
    "def update_weights(m_src,m_dest,epochNum):\n",
    "    sum_val_prev=sum_till_n(epochNum+1)\n",
    "    sum_val=sum_till_n(epochNum+2)\n",
    "    # print(sum_val_prev,sum_val)\n",
    "    # as epochNum starts from 1\n",
    "    # we consider that the epochnum starts from 0\n",
    "    print_turn=0\n",
    "    for p_src,q_dest in zip(m_src.parameters(), m_dest.parameters()):\n",
    "        \n",
    "        if p_src.requires_grad and q_dest.requires_grad:\n",
    "            # print(p_src.data.shape,q_dest.data.shape)\n",
    "            prev_total=q_dest.data*sum_val_prev\n",
    "            new_vals=p_src.data*(epochNum+1)\n",
    "            new_average=(prev_total+new_vals)/sum_val\n",
    "            q_dest.data=new_average\n",
    "            # if print_turn==0:\n",
    "            #     # print(sum_val_prev,sum_val)\n",
    "            #     print(p_src.data[0][0])\n",
    "            #     print(new_average[0][0])\n",
    "            # print_turn+=1\n",
    "                        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fce3678b-7bcc-4bba-b558-da7fe1ef8c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 10,788,929 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "\n",
    "n_embd  = 384                  ## every id gets embedded to vector of this size\n",
    "n_head  = 6\n",
    "n_layer = 6\n",
    "dropout = 0.0\n",
    "block_size = 256      ## max content length for predictions\n",
    "learning_rate = 3e-4             ## 0.001\n",
    "model   = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "optimizer = torch.optim.Adam(  m.parameters(), lr=learning_rate   )\n",
    "\n",
    "######################################################################\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a3a70d-988f-4f59-82e3-dd1e7c67709f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004627892166126962\n"
     ]
    }
   ],
   "source": [
    "# print(calc_compression(model))\n",
    "# for p in model.parameters(): \n",
    "#     if p.requires_grad:\n",
    "#         p.data=torch.zeros(p.shape)\n",
    "\n",
    "print(calc_compression(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0d5f112-f154-4c67-bec9-83c5cfc8dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64 \n",
    "max_iters  = 5000\n",
    "eval_interval = 10\n",
    "\n",
    "\n",
    "eval_iters = 200\n",
    "vocab_size = 65\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21e41ae-6ef5-4383-b2ec-12fd1ad7d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Parameters and their sizes:\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(f\"{name}: {param.size()}\")\n",
    "\n",
    "\n",
    "# for p in model.parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13342458-61d0-4ea2-887b-ff4026859fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "model_clone = BigramLanguageModel()\n",
    "model_clone.load_state_dict(copy.deepcopy(m.state_dict()))\n",
    "\n",
    "\n",
    "\n",
    "set_zero(model_clone)\n",
    "model_clone=model_clone.to(device)\n",
    "print(calc_compression(model_clone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2fbbd-aaff-46f6-b1d2-1572210f0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter in range(max_iters):\n",
    "    # if iter % eval_interval == 0:\n",
    "    #     losses = estimate_loss()\n",
    "    #     print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    # start=time.time()\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    ## evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)   ## zero out\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    " \n",
    "    update_weights(m,model_clone,iter)\n",
    "    # end=time.time()\n",
    "    # print(\"one iter\",end-start)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f722b32-615e-4b5d-9383-1a9495a33fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to save the model parameters\n",
    "model_path = f'model_parameters_{max_iters}.pth'\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(m.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b167c-049d-4ae5-b693-bd93502773d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to save the model parameters\n",
    "model_path = f'model_gradient_parameters_{max_iters}.pth'\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(model_clone.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c75fc0-9249-40c2-b590-26964b07d139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d454e78-76eb-48de-a657-f91e28c0161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Second Citizen:\n",
      "Ay, buckins, service; for we will do this,\n",
      "Your presented where he we shall be breathed him.\n",
      "\n",
      "First Murderer:\n",
      "I think his face, I have dream'd offended.\n",
      "\n",
      "Second Murderer:\n",
      "Ho, ho! a poor gentlewoman: what he doth good\n",
      "With one of you all this convoice, 'tis no limp\n",
      "So take with him before as the gates all,\n",
      "As 'tis like i' the gap of lime?\n",
      "\n",
      "Second Musician:\n",
      "Pray it well.\n",
      "\n",
      "Prieve it is your noise, Juliet.\n",
      "Take you not these partnical nice your parts.\n",
      "\n",
      "Huntsman:\n",
      "I truly, through\n",
      "y wi\n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "#### now, regenerate after some training\n",
    "\n",
    "\n",
    "## Kick off generation with some starting token. In this case id 0\n",
    "\n",
    "context = torch.zeros(  (2, 2),  dtype=torch.long, device=device   )\n",
    "\n",
    "gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(  decode(gen_text)   )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e797ecc-78d8-4136-a44b-fe885eb95a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [0, 0]], device='cuda:0')\n",
      " \n",
      "BUCKINGHAM:\n",
      "Then, his life is better in the face,\n",
      "When I must speak alour for him, your are none.\n",
      "\n",
      "BLUCKIO:\n",
      "One word, O, would I were such the gross\n",
      "So moodesty achieved a screw.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Tis most so lovely her?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Loved her, on my lord, and Montague,\n",
      "To this worthy servant with herself companies\n",
      "To taught up her before to lose her proud.\n",
      "\n",
      "ISABELLA:\n",
      "\n",
      "All her both.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "'Tis not men to her; this is the Tower.\n",
      "\n",
      "LUCIO:\n",
      "Not a hury business, but I shall quit his foo\n"
     ]
    }
   ],
   "source": [
    "context[0][0]=1\n",
    "print(context)\n",
    "gen_text = m.generate(context, max_new_tokens=500)[0].tolist()\n",
    "\n",
    "print(  decode(gen_text)   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77269e1-bc39-4c10-ba24-8988a8b615c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a72a7ad6-4d34-4ae9-b06c-09f600af0f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split val\n",
      " val loss {'val': tensor(2.0060)}\n"
     ]
    }
   ],
   "source": [
    "losses = estimate_loss(only_validation=True)\n",
    "print(f\" val loss {losses}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7ae7f-b9ae-47e8-a66e-ef5f77bef903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamapy310Kern",
   "language": "python",
   "name": "llamapy310kern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
